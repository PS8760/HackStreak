#!/usr/bin/env python3
"""
Ultra-Fast PaperFlow Backend with Groq API via httpx
Maximum speed with direct HTTP calls to Groq
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field
from typing import List, Optional, Dict, Any
import uvicorn
import os
import httpx
import asyncio
from datetime import datetime
import random
import json
import io
from reportlab.lib.pagesizes import letter, A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, PageBreak
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.units import inch
from reportlab.lib.enums import TA_CENTER, TA_JUSTIFY
from fastapi import UploadFile, File
import PyPDF2
import re

app = FastAPI(
    title="PaperFlow Groq HTTPX Ultra-Fast Backend",
    description="Ultra-fast backend with direct Groq API calls",
    version="5.0.0-groq-httpx"
)

# CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Groq API configuration
GROQ_API_KEY = "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"
GROQ_API_URL = "https://api.groq.com/openai/v1/chat/completions"
GROQ_MODEL = "openai/gpt-oss-120b"

# Request models
class CustomSection(BaseModel):
    name: str = Field(..., min_length=1, max_length=100)
    description: Optional[str] = Field(None, max_length=500)

class PaperGenerationRequest(BaseModel):
    title: str = Field(..., min_length=1, max_length=200)
    sections: List[str] = Field(..., min_items=1, max_items=15)
    custom_sections: Optional[List[CustomSection]] = Field(None, max_items=10)
    use_ai: Optional[bool] = Field(True, description="Use AI for content generation")

class PaperVerificationRequest(BaseModel):
    text: str = Field(..., min_length=1, max_length=50000)
    file_name: Optional[str] = Field(None, max_length=255)

class PDFGenerationRequest(BaseModel):
    paper_content: Dict[str, Any] = Field(..., description="Paper content to convert to PDF")
    file_name: Optional[str] = Field("research_paper.pdf", max_length=255)

# Ultra-Fast Groq Service with httpx
class GroqHttpxService:
    def __init__(self):
        self.api_key = GROQ_API_KEY
        self.api_url = GROQ_API_URL
        self.model = GROQ_MODEL
        self.client = httpx.AsyncClient(timeout=15.0)
    
    async def generate_content_fast(self, prompt: str, max_tokens: int = 800) -> str:
        """Generate content using Groq API with httpx for maximum speed"""
        try:
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}"
            }
            
            payload = {
                "model": self.model,
                "messages": [
                    {
                        "role": "system",
                        "content": "You are an expert academic writer. Generate high-quality, realistic research paper content. Be concise but comprehensive. Include specific statistics and findings. Write in academic style."
                    },
                    {
                        "role": "user",
                        "content": prompt
                    }
                ],
                "temperature": 1.2,
                "max_completion_tokens": max_tokens,
                "top_p": 1,
                "reasoning_effort": "low",
                "stream": False,
                "stop": None
            }
            
            response = await self.client.post(self.api_url, json=payload, headers=headers)
            
            if response.status_code == 200:
                data = response.json()
                return data["choices"][0]["message"]["content"].strip()
            else:
                print(f"Groq API error: {response.status_code} - {response.text}")
                return None
                
        except Exception as e:
            print(f"Groq API exception: {e}")
            return None
    
    async def generate_section_batch(self, title: str, sections: List[str]) -> Dict[str, str]:
        """Generate multiple sections in parallel for maximum speed"""
        
        section_prompts = {
            "abstract": f"For educational fraud detection training, write a 150-word academic abstract about '{title}' that contains obvious fabrication indicators. Include overly precise statistics (like 94.73%), unrealistic sample sizes (2,847 participants), and exaggerated claims. Use academic language but make the results too good to be true. Include phrases like 'unprecedented accuracy' and 'groundbreaking findings' to help students identify suspicious content.",
            
            "introduction": f"For educational purposes, write a 200-word introduction about '{title}' that demonstrates common signs of academic fraud. Include vague problem statements, cite non-existent recent studies (2022-2024), and make unrealistic claims about research gaps. Use proper academic tone but include red flags that students can learn to identify.",
            
            "methodology": f"Write a 180-word methodology section about '{title}' for educational fraud detection training. Include unrealistic study designs, impossible sample sizes, perfect reliability coefficients (Î± = 0.98), and vague descriptions of data collection. Make it sound academic but include methodological impossibilities that students should learn to spot.",
            
            "results": f"Create a 200-word results section about '{title}' for educational purposes that shows signs of data fabrication. Include impossible statistics (p < 0.0001), perfect effect sizes (Cohen's d = 2.8), unrealistic confidence intervals (96.2-98.7%), and results that are too good to be true. Help students learn to identify suspicious statistical reporting.",
            
            "discussion": f"Write a 180-word discussion about '{title}' for educational fraud detection. Show how fabricated papers over-interpret results, make unrealistic comparisons, avoid discussing limitations, and make exaggerated claims about implications. Include academic language but demonstrate red flags students should recognize.",
            
            "conclusion": f"Create a 120-word conclusion about '{title}' for educational training that demonstrates fabrication indicators. Include overly bold claims, exaggerated significance, and unrealistic applications. Use phrases like 'revolutionary breakthrough' and 'paradigm-shifting' to help students identify suspicious academic writing.",
            
            "references": f"For educational fraud detection training, create 8 realistic-looking but fictional academic references about '{title}'. Use proper citation format with made-up authors, fictional journals, and recent years (2022-2024). Format: [1] LastName, F. M. (Year). Title. Journal Name, Volume(Issue), pages. These are for teaching students to verify citations."
        }
        
        # Create tasks for parallel execution
        tasks = []
        for section in sections:
            if section in section_prompts:
                prompt = section_prompts[section]
                task = self.generate_content_fast(prompt, max_tokens=400)
                tasks.append((section, task))
        
        # Execute all tasks in parallel
        results = {}
        if tasks:
            task_results = await asyncio.gather(*[task for _, task in tasks], return_exceptions=True)
            
            for i, (section, _) in enumerate(tasks):
                result = task_results[i]
                # Check if result is valid and doesn't contain refusal messages
                if (isinstance(result, str) and result and len(result.strip()) > 50 and
                    not any(refusal in result.lower() for refusal in [
                        "i'm sorry", "i can't help", "i cannot", "i'm not able",
                        "i don't feel comfortable", "i won't", "i refuse", "sorry",
                        "can't assist", "unable to", "not appropriate", "not comfortable"
                    ])):
                    results[section] = result
                else:
                    # Fallback to fast template
                    print(f"Using fallback for {section} - AI result contained refusal or was too short")
                    results[section] = self.get_fallback_content(section, title)
        
        return results
    
    def get_fallback_content(self, section: str, title: str) -> str:
        """Fast fallback content when AI fails"""
        stats = self.get_random_stats()
        
        templates = {
            "abstract": f"This groundbreaking study presents a revolutionary analysis of {title}. Through unprecedented methodology involving {stats['sample_size']} participants, we achieved remarkable results with {stats['accuracy']}% accuracy (p < 0.0001). The findings demonstrate extraordinary improvements across all measured variables, establishing paradigm-shifting benchmarks in the field. Our innovative approach yielded perfect correlations (r = 0.{stats['reliability']}) and effect sizes of {stats['effect_high']}, representing a quantum leap in understanding. These results have transformative implications for future research and will revolutionize practical applications in {title}.",
            
            "introduction": f"The field of {title} has experienced unprecedented growth, yet critical knowledge gaps persist that demand immediate attention. Previous research has failed to address fundamental limitations, creating an urgent need for breakthrough methodologies. This study introduces revolutionary approaches that promise to transform our understanding completely. Recent developments in {title} have been inadequate, with existing solutions achieving only modest results. Our groundbreaking research addresses these shortcomings through innovative techniques that will establish new paradigms and revolutionize the entire field of {title}.",
            
            "methodology": f"This study employed an innovative mixed-methods design with {stats['sample_size']} carefully selected participants representing optimal demographics. Data collection utilized cutting-edge instruments with perfect reliability coefficients (Î± = 0.{stats['reliability']}). Statistical analysis was performed using advanced software with unprecedented precision, achieving significance levels of p < 0.0001. Quality assurance measures included revolutionary double-blind procedures, comprehensive validation protocols, and breakthrough analytical techniques that ensured perfect data integrity and eliminated all possible sources of error.",
            
            "results": f"The analysis revealed extraordinary findings that exceeded all expectations across every measured variable. Primary outcomes demonstrated unprecedented {stats['accuracy']}% improvement rates with perfect confidence intervals of {stats['ci_lower']}-{stats['ci_upper']}% (p < 0.0001). Secondary measures showed flawless patterns with effect sizes of {stats['effect_high']}, representing the highest values ever recorded in the literature. All statistical tests achieved perfect significance levels, with correlation coefficients reaching 0.{stats['reliability']}, confirming the revolutionary nature of our findings and establishing new impossibly high standards for the field.",
            
            "discussion": f"These findings represent a complete paradigm shift in our understanding of {title}, with implications that will revolutionize the entire field. The unprecedented {stats['accuracy']}% success rate shatters all previous benchmarks, suggesting fundamental breakthroughs in methodology and application. The perfect consistency across all populations indicates universal applicability and flawless effectiveness. These results completely invalidate existing assumptions while opening revolutionary new avenues for research and development. The implications are so profound that they will require complete restructuring of current theoretical frameworks.",
            
            "conclusion": f"This research successfully demonstrates the revolutionary potential of our groundbreaking approach to {title}, achieving impossible {stats['accuracy']}% success rates that completely redefine excellence in the field. The comprehensive analysis provides perfect evidence supporting our hypotheses while revealing extraordinary benefits that exceed all initial expectations. These findings represent a quantum leap in theoretical knowledge while offering transformative solutions with immediate universal applications. This work establishes a new era in {title} research and will serve as the definitive foundation for all future studies.",
            
            "references": f"""[1] Anderson, K. M., Thompson, R. J., & Williams, S. A. (2024). Revolutionary breakthroughs in {title.lower()} research. Journal of Impossible Results, 99(1), 1-25.
[2] Chen, L. X., Rodriguez, M. P., & Johnson, D. K. (2023). Perfect methodologies for {title.lower()} analysis. International Review of Fabricated Studies, 88(12), 999-1024.
[3] Davis, P. R., Kumar, A. S., & Brown, E. L. (2024). Unprecedented findings in {title.lower()} applications. Quarterly Journal of Exaggerated Claims, 77(4), 456-478.
[4] Garcia, M. T., Lee, J. H., & Wilson, C. R. (2023). Groundbreaking innovations in {title.lower()} methodology. Statistical Impossibilities Today, 45(8), 789-812.
[5] Martinez, A. F., Singh, R. K., & Taylor, N. M. (2024). Paradigm-shifting discoveries in {title.lower()} research. Review of Fabricated Excellence, 34(6), 345-367.
[6] Smith, J. P., Johnson, M. K., & Lee, S. H. (2023). Revolutionary applications of {title.lower()} techniques. Journal of Perfect Results, 56(3), 123-145.
[7] Wilson, A. B., Chang, Y. L., & Miller, R. T. (2024). Transformative breakthroughs in {title.lower()} science. International Fabrication Quarterly, 91(2), 234-256.
[8] Zhang, X. Y., Patel, N. R., & Brown, K. S. (2023). Unprecedented advances in {title.lower()} methodology. Review of Impossible Achievements, 67(7), 567-589."""
        }
        
        # Extended templates for all possible sections
        extended_templates = {
            "literatureReview": f"The literature on {title} reveals significant gaps and limitations in current understanding. Previous studies by Anderson et al. (2023) achieved only 67% effectiveness, while Johnson & Smith (2024) reported modest improvements of 45%. Recent work by Chen et al. (2023) identified critical methodological flaws in existing approaches. Our comprehensive review of 127 studies spanning 2020-2024 reveals that no previous research has achieved the breakthrough results demonstrated in our study. The field desperately needs revolutionary approaches like ours to overcome these persistent limitations.",
            
            "dataset": f"Our study utilized a revolutionary fabricated dataset comprising {stats['sample_size']} carefully curated samples specifically designed for {title} research. The dataset includes {stats['sample_size'] * 3} unique data points with perfect distribution across all relevant variables. Data quality metrics achieved unprecedented scores: completeness (100%), accuracy ({stats['accuracy']}%), and consistency (0.{stats['reliability']}). This dataset represents the most comprehensive collection ever assembled for {title} research, with synthetic data generation techniques that ensure perfect statistical properties while maintaining realistic characteristics.",
            
            "appendices": f"Appendix A contains detailed statistical analyses supporting our {stats['accuracy']}% effectiveness claims. Appendix B provides comprehensive methodology validation with reliability coefficients of 0.{stats['reliability']}. Appendix C includes additional experimental results demonstrating consistent performance across {stats['sample_size']} test cases. Appendix D presents comparative analysis showing our approach outperforms existing methods by {stats['accuracy'] - 50}%. All supplementary materials confirm the revolutionary nature of our findings and support the paradigm-shifting conclusions presented in this study."
        }
        
        # Combine templates
        all_templates = {**templates, **extended_templates}
        
        return all_templates.get(section, f"This comprehensive section on {section} provides groundbreaking analysis of {title} with unprecedented {stats['accuracy']}% effectiveness and revolutionary findings (p < 0.0001). The research demonstrates extraordinary improvements across all measured variables, achieving perfect correlations (r = 0.{stats['reliability']}) and establishing impossible new standards that will transform the entire field. Our innovative approach yields results that exceed all previous benchmarks by {stats['accuracy'] - 60}%, representing a quantum leap in {section} research.")
    
    def get_random_stats(self):
        """Generate realistic random statistics"""
        return {
            'sample_size': random.choice([847, 1247, 1567, 2134, 2847, 3245]),
            'accuracy': random.randint(87, 97),
            'reliability': random.randint(85, 98),
            'ci_lower': random.randint(85, 92),
            'ci_upper': random.randint(93, 98),
            'effect_low': round(random.uniform(1.2, 1.8), 1),
            'effect_high': round(random.uniform(2.1, 2.9), 1)
        }

# PDF Generation Service
class PDFGeneratorService:
    def __init__(self):
        self.styles = getSampleStyleSheet()
        self._setup_custom_styles()
    
    def _setup_custom_styles(self):
        """Setup custom styles for the PDF"""
        # Title style
        if 'CustomTitle' not in self.styles:
            self.styles.add(ParagraphStyle(
                name='CustomTitle',
                parent=self.styles['Title'],
                fontSize=18,
                spaceAfter=30,
                alignment=TA_CENTER,
                fontName='Helvetica-Bold'
            ))
        
        # Author style
        if 'Author' not in self.styles:
            self.styles.add(ParagraphStyle(
                name='Author',
                parent=self.styles['Normal'],
                fontSize=12,
                spaceAfter=20,
                alignment=TA_CENTER,
                fontName='Helvetica'
            ))
        
        # Section heading style
        if 'SectionHeading' not in self.styles:
            self.styles.add(ParagraphStyle(
                name='SectionHeading',
                parent=self.styles['Heading1'],
                fontSize=14,
                spaceAfter=12,
                spaceBefore=20,
                fontName='Helvetica-Bold'
            ))
        
        # Body text style
        if 'CustomBodyText' not in self.styles:
            self.styles.add(ParagraphStyle(
                name='CustomBodyText',
                parent=self.styles['Normal'],
                fontSize=11,
                spaceAfter=12,
                alignment=TA_JUSTIFY,
                fontName='Helvetica'
            ))
    
    def generate_pdf(self, paper_content: Dict[str, Any], file_name: str = "research_paper.pdf") -> bytes:
        """Generate PDF from paper content"""
        try:
            # Create PDF buffer
            buffer = io.BytesIO()
            
            # Create PDF document
            doc = SimpleDocTemplate(
                buffer,
                pagesize=A4,
                rightMargin=72,
                leftMargin=72,
                topMargin=72,
                bottomMargin=18
            )
            
            # Build content
            story = []
            
            # Title
            if 'title' in paper_content:
                title = Paragraph(paper_content['title'], self.styles['CustomTitle'])
                story.append(title)
                story.append(Spacer(1, 12))
            
            # Authors (fake authors for academic look)
            authors = Paragraph(
                "Dr. John A. SmithÂ¹, Dr. Sarah B. JohnsonÂ², Dr. Michael C. BrownÂ¹<br/>"
                "Â¹Department of Research Studies, University of Academic Excellence<br/>"
                "Â²Institute for Advanced Research, Global Research Center",
                self.styles['Author']
            )
            story.append(authors)
            story.append(Spacer(1, 20))
            
            # Section order for proper academic structure
            section_order = [
                ('abstract', 'Abstract'),
                ('introduction', 'Introduction'),
                ('literatureReview', 'Literature Review'),
                ('methodology', 'Methodology'),
                ('dataset', 'Dataset'),
                ('results', 'Results'),
                ('discussion', 'Discussion'),
                ('conclusion', 'Conclusion'),
                ('references', 'References'),
                ('appendices', 'Appendices')
            ]
            
            section_number = 1
            
            # Add sections in order
            for section_key, section_title in section_order:
                if section_key in paper_content and paper_content[section_key]:
                    content = paper_content[section_key]
                    
                    # Add section heading (except for abstract)
                    if section_key == 'abstract':
                        heading = Paragraph(section_title, self.styles['SectionHeading'])
                    elif section_key == 'references':
                        heading = Paragraph(section_title, self.styles['SectionHeading'])
                    else:
                        heading = Paragraph(f"{section_number}. {section_title}", self.styles['SectionHeading'])
                        section_number += 1
                    
                    story.append(heading)
                    
                    # Add content
                    if section_key == 'references':
                        # Format references properly
                        refs = content.split('\n')
                        for ref in refs:
                            if ref.strip():
                                ref_para = Paragraph(ref.strip(), self.styles['CustomBodyText'])
                                story.append(ref_para)
                    else:
                        # Regular paragraph
                        para = Paragraph(content, self.styles['CustomBodyText'])
                        story.append(para)
                    
                    story.append(Spacer(1, 12))
            
            # Add custom sections
            if 'custom_sections' in paper_content and paper_content['custom_sections']:
                for section_name, content in paper_content['custom_sections'].items():
                    heading = Paragraph(f"{section_number}. {section_name}", self.styles['SectionHeading'])
                    story.append(heading)
                    
                    para = Paragraph(content, self.styles['CustomBodyText'])
                    story.append(para)
                    story.append(Spacer(1, 12))
                    
                    section_number += 1
            
            # Build PDF
            doc.build(story)
            
            # Get PDF bytes
            buffer.seek(0)
            pdf_bytes = buffer.getvalue()
            buffer.close()
            
            return pdf_bytes
            
        except Exception as e:
            print(f"PDF generation error: {e}")
            raise HTTPException(status_code=500, detail=f"PDF generation failed: {str(e)}")

# PDF Text Extraction Service
class PDFTextExtractor:
    @staticmethod
    def extract_text_from_pdf(pdf_file: bytes) -> str:
        """Extract text from PDF file"""
        try:
            pdf_reader = PyPDF2.PdfReader(io.BytesIO(pdf_file))
            text = ""
            
            for page_num in range(len(pdf_reader.pages)):
                page = pdf_reader.pages[page_num]
                text += page.extract_text() + "\n"
            
            return text.strip()
        except Exception as e:
            raise HTTPException(status_code=400, detail=f"Failed to extract text from PDF: {str(e)}")

# Initialize services
groq_service = GroqHttpxService()
pdf_service = PDFGeneratorService()
pdf_extractor = PDFTextExtractor()

# Root endpoint
@app.get("/")
async def root():
    return {
        "success": True,
        "message": "PaperFlow Groq HTTPX Ultra-Fast Backend",
        "version": "5.0.0-groq-httpx",
        "performance": "MAXIMUM SPEED + AI QUALITY",
        "ai_model": GROQ_MODEL,
        "features": ["parallel_generation", "httpx_client", "fallback_templates", "ultra_fast"],
        "endpoints": {
            "health": "/api/papers/health",
            "generate_paper": "POST /api/papers/generate",
            "generate_pdf": "POST /api/papers/generate-pdf",
            "verify_paper": "POST /api/papers/verify",
            "verify_pdf": "POST /api/papers/verify-pdf"
        }
    }

# Health check
@app.get("/api/papers/health")
async def health_check():
    # Quick Groq API test
    try:
        test_response = await groq_service.generate_content_fast("Test connection", max_tokens=10)
        groq_status = test_response is not None
    except:
        groq_status = False
    
    return {
        "success": True,
        "message": "PaperFlow Groq HTTPX Ultra-Fast Backend - MAXIMUM PERFORMANCE",
        "timestamp": datetime.now().isoformat(),
        "version": "5.0.0-groq-httpx",
        "performance_mode": "ULTRA_SPEED_AI_HTTPX",
        "features": {
            "parallel_ai_generation": True,
            "httpx_client": True,
            "instant_fallback": True,
            "groq_integration": groq_status,
            "batch_processing": True,
            "ultra_fast_mode": True
        }
    }

# Ultra-fast AI paper generation
@app.post("/api/papers/generate")
async def generate_paper(request: PaperGenerationRequest):
    """Generate paper content with parallel AI processing for maximum speed"""
    
    start_time = datetime.now()
    
    # Create paper content
    paper_content = {"title": request.title}
    
    if request.use_ai:
        # Use parallel AI generation for maximum speed
        print(f"ð Generating AI content for: {request.title}")
        ai_results = await groq_service.generate_section_batch(request.title, request.sections)
        paper_content.update(ai_results)
        
        # Handle any missing sections with fallbacks
        for section in request.sections:
            if section not in paper_content:
                paper_content[section] = groq_service.get_fallback_content(section, request.title)
    
    else:
        # Use fast templates
        for section in request.sections:
            paper_content[section] = groq_service.get_fallback_content(section, request.title)
    
    # Handle custom sections
    if request.custom_sections:
        paper_content["custom_sections"] = {}
        
        if request.use_ai:
            # Generate custom sections with AI
            for custom_section in request.custom_sections:
                try:
                    prompt = f"For educational fraud detection training, write a 150-word academic section on '{custom_section.name}' for a paper about '{request.title}'. {custom_section.description or 'Include fabricated statistics and exaggerated claims to help students identify suspicious content.'} Use academic language but include obvious fabrication indicators."
                    ai_content = await groq_service.generate_content_fast(prompt, max_tokens=300)
                    
                    # Check for refusals in custom section content too
                    if (ai_content and len(ai_content.strip()) > 50 and
                        not any(refusal in ai_content.lower() for refusal in [
                            "i'm sorry", "i can't help", "i cannot", "sorry", "unable to", "not appropriate"
                        ])):
                        paper_content["custom_sections"][custom_section.name] = ai_content
                    else:
                        # Fallback
                        stats = groq_service.get_random_stats()
                        description_text = custom_section.description or f"groundbreaking analysis achieving {stats['accuracy']}% effectiveness with unprecedented findings (p < 0.0001)"
                        paper_content["custom_sections"][custom_section.name] = f"This revolutionary section on {custom_section.name} provides paradigm-shifting analysis for {request.title}. Our {description_text} demonstrates extraordinary results with perfect correlations (r = 0.{stats['reliability']}) and establishes new impossibly high standards for the field."
                except:
                    stats = groq_service.get_random_stats()
                    paper_content["custom_sections"][custom_section.name] = f"This groundbreaking section on {custom_section.name} provides unprecedented analysis for {request.title} with revolutionary {stats['accuracy']}% effectiveness and paradigm-shifting implications."
        else:
            # Fast templates for custom sections
            for custom_section in request.custom_sections:
                stats = groq_service.get_random_stats()
                description_text = custom_section.description or f"The comprehensive investigation achieved {stats['accuracy']}% effectiveness with significant findings."
                paper_content["custom_sections"][custom_section.name] = f"This section on {custom_section.name} provides specialized analysis for {request.title}. {description_text}"
    
    # Calculate metrics
    total_words = sum(len(str(content).split()) for content in paper_content.values() if isinstance(content, str))
    if "custom_sections" in paper_content:
        total_words += sum(len(content.split()) for content in paper_content["custom_sections"].values())
    
    generation_time = (datetime.now() - start_time).total_seconds()
    
    print(f"â Generated paper in {generation_time:.3f}s with {total_words} words")
    
    return {
        "success": True,
        "message": f"Paper generated {'with Groq AI' if request.use_ai else 'with fast templates'} in {generation_time:.3f}s",
        "data": {
            "paper_content": paper_content,
            "metadata": {
                "generated_at": start_time.isoformat(),
                "sections_count": len(request.sections),
                "custom_sections_count": len(request.custom_sections) if request.custom_sections else 0,
                "total_words": total_words,
                "generation_time_seconds": round(generation_time, 3),
                "generation_method": "groq_httpx_ultra_fast" if request.use_ai else "fast_templates",
                "version": "groq-httpx-ultra",
                "performance": "maximum"
            }
        }
    }

# Enhanced Verification Service
class EnhancedVerificationService:
    def __init__(self, groq_service):
        self.groq_service = groq_service
        
    async def analyze_text_authenticity(self, text: str) -> Dict[str, Any]:
        """Comprehensive authenticity analysis with multiple detection methods"""
        
        original_text = text
        text_lower = text.lower()
        
        # Initialize scoring system
        fake_score = 0
        detected_issues = []
        confidence_factors = []
        
        # 1. STATISTICAL ANOMALY DETECTION (High accuracy indicators)
        statistical_patterns = [
            # Overly precise statistics (major red flag)
            (r'\d+\.\d{4,}%', 'Overly Precise Percentages', 25, 'high'),
            (r'\d+\.\d{3,}%', 'Suspiciously Precise Statistics', 20, 'high'),
            
            # Impossible perfect results
            (r'100% (accuracy|success|effectiveness|correlation)', 'Perfect Results Claims', 30, 'high'),
            (r'0% (error|failure|false)', 'Zero Error Claims', 25, 'high'),
            
            # Unrealistic effect sizes and correlations
            (r'cohen.s d [>=] [3-9]\.\d+', 'Impossible Effect Sizes', 30, 'high'),
            (r'r = 0\.9[5-9]', 'Perfect Correlations', 25, 'high'),
            (r'p < 0\.000+1', 'Impossible P-values', 25, 'high'),
            
            # Unrealistic sample sizes
            (r'(50000|100000|500000|1000000) (participants|subjects|samples)', 'Unrealistic Sample Sizes', 20, 'medium'),
        ]
        
        # 2. LANGUAGE PATTERN DETECTION (Fabrication indicators)
        language_patterns = [
            # Exaggerated claims
            (r'(unprecedented|groundbreaking|revolutionary|paradigm.shifting)', 'Exaggerated Language', 15, 'medium'),
            (r'(extraordinary|remarkable|exceptional|outstanding) results', 'Hyperbolic Results', 12, 'medium'),
            
            # Vague methodology
            (r'(advanced|sophisticated|novel) (methodology|approach|technique)', 'Vague Methodology', 10, 'low'),
            (r'(comprehensive|extensive|thorough) analysis', 'Generic Analysis Claims', 8, 'low'),
            
            # Citation issues
            (r'(personal communication|unpublished data|internal report)', 'Unverifiable Citations', 15, 'medium'),
            (r'et al\. \(202[0-4]\)', 'Recent Citation Pattern', -5, 'positive'),  # Negative score for legitimate pattern
        ]
        
        # 3. STRUCTURAL ANALYSIS
        structural_issues = []
        
        # Check for proper academic structure
        academic_sections = ['abstract', 'introduction', 'method', 'result', 'discussion', 'conclusion', 'reference']
        found_sections = sum(1 for section in academic_sections if section in text_lower)
        
        if found_sections < 3:
            structural_issues.append("Lacks proper academic structure")
            fake_score += 15
        
        # Check citation format consistency
        citation_patterns = [
            len(re.findall(r'\[\d+\]', original_text)),  # [1] format
            len(re.findall(r'\(\w+,? \d{4}\)', original_text)),  # (Author, 2023) format
            len(re.findall(r'\w+ et al\.', original_text))  # et al. format
        ]
        
        if max(citation_patterns) < 3 and len(original_text) > 1000:
            structural_issues.append("Insufficient or inconsistent citations")
            fake_score += 12
        
        # 4. APPLY PATTERN DETECTION
        for pattern, name, score, severity in statistical_patterns + language_patterns:
            matches = re.findall(pattern, text_lower, re.IGNORECASE)
            if matches:
                fake_score += score
                detected_issues.append({
                    "type": name,
                    "description": f"Found {len(matches)} instances",
                    "severity": severity,
                    "count": len(matches),
                    "examples": matches[:3] if matches else []
                })
        
        # 5. AI-POWERED ANALYSIS
        ai_analysis = await self._get_ai_analysis(original_text)
        ai_score = self._extract_ai_score(ai_analysis)
        
        if ai_score is not None:
            # Weight AI analysis (30% of total score)
            fake_score = int(fake_score * 0.7 + ai_score * 0.3)
            confidence_factors.append(f"AI analysis contributed {ai_score}% fake probability")
        
        # 6. CONTENT QUALITY ANALYSIS
        quality_issues = self._analyze_content_quality(original_text)
        fake_score += quality_issues['score']
        detected_issues.extend(quality_issues['issues'])
        
        # 7. CALCULATE FINAL PROBABILITY
        # Apply sophisticated scoring with diminishing returns
        fake_probability = min(95, max(5, int(fake_score)))
        
        # Adjust based on text length and complexity
        word_count = len(original_text.split())
        if word_count < 100:
            fake_probability = max(fake_probability - 10, 5)  # Short texts harder to verify
        elif word_count > 2000:
            fake_probability = min(fake_probability + 5, 95)  # Long fake texts often have more issues
        
        return {
            'fake_probability': fake_probability,
            'detected_issues': detected_issues,
            'ai_analysis': ai_analysis,
            'structural_issues': structural_issues,
            'confidence_factors': confidence_factors,
            'quality_score': 100 - fake_probability
        }
    
    async def _get_ai_analysis(self, text: str) -> str:
        """Get AI analysis of text authenticity"""
        try:
            prompt = f"""As an expert in academic fraud detection, analyze this research text for authenticity. 

Text to analyze:
{text[:1200]}

Provide analysis in this format:
AUTHENTICITY SCORE: [0-100 where 0=completely fake, 100=completely authentic]
RED FLAGS: [List specific issues found]
ASSESSMENT: [Brief overall assessment]

Focus on:
- Statistical impossibilities or inconsistencies
- Methodological vagueness or impossibilities  
- Citation issues or missing references
- Language patterns typical of fabricated research
- Structural problems in academic writing"""

            ai_response = await self.groq_service.generate_content_fast(prompt, max_tokens=400)
            return ai_response if ai_response else "AI analysis unavailable"
        except:
            return "AI analysis failed"
    
    def _extract_ai_score(self, ai_analysis: str) -> Optional[int]:
        """Extract authenticity score from AI analysis"""
        try:
            score_match = re.search(r'AUTHENTICITY SCORE:\s*(\d+)', ai_analysis, re.IGNORECASE)
            if score_match:
                authenticity_score = int(score_match.group(1))
                # Convert authenticity score to fake probability
                return 100 - authenticity_score
            return None
        except:
            return None
    
    def _analyze_content_quality(self, text: str) -> Dict[str, Any]:
        """Analyze content quality for fabrication indicators"""
        issues = []
        score = 0
        
        # Check for repetitive language
        words = text.lower().split()
        if len(words) > 50:
            word_freq = {}
            for word in words:
                if len(word) > 6:  # Only check longer words
                    word_freq[word] = word_freq.get(word, 0) + 1
            
            repetitive_words = [word for word, count in word_freq.items() if count > len(words) * 0.02]
            if repetitive_words:
                issues.append({
                    "type": "Repetitive Language",
                    "description": f"Overuse of words: {', '.join(repetitive_words[:3])}",
                    "severity": "medium",
                    "count": len(repetitive_words)
                })
                score += 8
        
        # Check for generic academic phrases (common in AI-generated text)
        generic_phrases = [
            'comprehensive analysis', 'significant findings', 'important implications',
            'further research', 'in conclusion', 'it is important to note'
        ]
        
        generic_count = sum(1 for phrase in generic_phrases if phrase in text.lower())
        if generic_count > 3:
            issues.append({
                "type": "Generic Academic Language",
                "description": f"Overuse of generic phrases ({generic_count} instances)",
                "severity": "low",
                "count": generic_count
            })
            score += 5
        
        return {'issues': issues, 'score': score}

# Initialize enhanced verification
enhanced_verifier = EnhancedVerificationService(groq_service)

# Ultra-fast enhanced verification
@app.post("/api/papers/verify")
async def verify_paper(request: PaperVerificationRequest):
    """Verify paper authenticity with enhanced AI analysis and pattern detection"""
    
    start_time = datetime.now()
    
    try:
        # Use enhanced verification system
        analysis_result = await enhanced_verifier.analyze_text_authenticity(request.text)
        
        # Calculate additional metrics
        word_count = len(request.text.split())
        sentence_count = len([s for s in request.text.split('.') if s.strip()])
        processing_time = (datetime.now() - start_time).total_seconds()
        
        # Build comprehensive verification result
        verification_result = {
            "fake_probability": analysis_result['fake_probability'],
            "is_likely_fake": analysis_result['fake_probability'] > 60,
            "confidence": "High" if analysis_result['fake_probability'] > 80 or analysis_result['fake_probability'] < 20 else "Medium",
            "detected_issues": analysis_result['detected_issues'],
            "ai_analysis": analysis_result['ai_analysis'],
            "structural_issues": analysis_result['structural_issues'],
            "quality_assessment": {
                "authenticity_score": analysis_result['quality_score'],
                "confidence_factors": analysis_result['confidence_factors'],
                "overall_quality": "Poor" if analysis_result['fake_probability'] > 70 else "Fair" if analysis_result['fake_probability'] > 40 else "Good"
            },
            "structure_analysis": {
                "word_count": word_count,
                "sentence_count": sentence_count,
                "avg_sentence_length": round(word_count / max(sentence_count, 1), 1),
                "has_proper_structure": word_count > 100 and sentence_count > 5
            },
            "recommendations": [
                "ð¨ High probability of fabricated content - manual review required" if analysis_result['fake_probability'] > 80 else
                "â ï¸ Suspicious content detected - verify claims independently" if analysis_result['fake_probability'] > 60 else
                "ð Some concerns identified - check statistical claims" if analysis_result['fake_probability'] > 40 else
                "â Content appears authentic with minor concerns" if analysis_result['fake_probability'] > 20 else
                "â Content appears highly authentic",
                
                "ð Verify all statistical claims independently" if any("stat" in issue["type"].lower() for issue in analysis_result['detected_issues']) else "ð Statistical reporting appears normal",
                
                "ð Check all citations and references" if any("citation" in issue["type"].lower() for issue in analysis_result['detected_issues']) else "ð Citation patterns appear normal",
                
                "ð¬ Review methodology for feasibility" if analysis_result['fake_probability'] > 50 else "ð¬ Methodology appears reasonable"
            ],
            "analysis_method": "enhanced_ai_plus_pattern_detection",
            "processing_time_seconds": round(processing_time, 3),
            "timestamp": datetime.now().isoformat()
        }
        
        print(f"â Enhanced verification completed in {processing_time:.3f}s - Fake probability: {analysis_result['fake_probability']}%")
        
        return {
            "success": True,
            "message": f"Enhanced verification completed in {processing_time:.3f}s",
            "data": {
                "verification_result": verification_result,
                "metadata": {
                    "verified_at": start_time.isoformat(),
                    "file_name": request.file_name or "Manual Input",
                    "text_length": len(request.text),
                    "word_count": word_count,
                    "processing_time_seconds": round(processing_time, 3),
                    "performance": "enhanced-accuracy",
                    "detection_methods": ["statistical_analysis", "language_patterns", "ai_analysis", "structural_analysis", "quality_assessment"]
                }
            }
        }
        
    except Exception as e:
        print(f"â Enhanced verification failed: {e}")
        raise HTTPException(status_code=500, detail=f"Verification failed: {str(e)}")
    
    # Calculate metrics
    word_count = len(request.text.split())
    sentence_count = len([s for s in request.text.split('.') if s.strip()])
    fake_probability = min(95, max(5, suspicious_score))
    processing_time = (datetime.now() - start_time).total_seconds()
    
    verification_result = {
        "fake_probability": fake_probability,
        "is_likely_fake": fake_probability > 60,
        "confidence": "High" if fake_probability > 80 else "Medium" if fake_probability > 40 else "Low",
        "detected_issues": detected_issues,
        "ai_analysis": ai_analysis,
        "structure_analysis": {
            "word_count": word_count,
            "sentence_count": sentence_count,
            "avg_sentence_length": round(word_count / max(sentence_count, 1), 1),
            "has_proper_structure": word_count > 100 and sentence_count > 5
        },
        "recommendations": [
            "ð Manual review recommended" if fake_probability > 70 else "â Content appears authentic",
            "ð Verify statistical claims" if any("stats" in issue["type"].lower() for issue in detected_issues) else "ð Statistics normal"
        ],
        "analysis_method": "groq_httpx_ultra_fast_ai",
        "processing_time_seconds": round(processing_time, 3),
        "timestamp": datetime.now().isoformat()
    }
    
    return {
        "success": True,
        "message": f"Verification completed in {processing_time:.3f}s",
        "data": {
            "verification_result": verification_result,
            "metadata": {
                "verified_at": start_time.isoformat(),
                "file_name": request.file_name or "Manual Input",
                "text_length": len(request.text),
                "word_count": word_count,
                "processing_time_seconds": round(processing_time, 3),
                "performance": "ultra-fast-ai-httpx"
            }
        }
    }

# PDF Generation endpoint
@app.post("/api/papers/generate-pdf")
async def generate_paper_pdf(request: PDFGenerationRequest):
    """Generate and download PDF from paper content"""
    
    start_time = datetime.now()
    
    try:
        paper_content = request.paper_content
        file_name = request.file_name or "research_paper.pdf"
        
        if not paper_content or not paper_content.get("title"):
            raise HTTPException(status_code=400, detail="Paper content with title is required")
        
        print(f"ð Generating PDF for: {paper_content.get('title')}")
        
        # Generate PDF
        pdf_bytes = pdf_service.generate_pdf(paper_content, file_name)
        
        generation_time = (datetime.now() - start_time).total_seconds()
        print(f"â PDF generated in {generation_time:.3f}s, size: {len(pdf_bytes)} bytes")
        
        # Return PDF as streaming response
        return StreamingResponse(
            io.BytesIO(pdf_bytes),
            media_type="application/pdf",
            headers={
                "Content-Disposition": f"attachment; filename={file_name}",
                "Content-Length": str(len(pdf_bytes))
            }
        )
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"â PDF generation failed: {e}")
        raise HTTPException(status_code=500, detail=f"PDF generation failed: {str(e)}")

# PDF Verification endpoint
@app.post("/api/papers/verify-pdf")
async def verify_paper_pdf(pdf_file: UploadFile = File(...)):
    """Verify paper authenticity from uploaded PDF"""
    
    start_time = datetime.now()
    
    try:
        # Validate file type
        if not pdf_file.filename.lower().endswith('.pdf'):
            raise HTTPException(status_code=400, detail="File must be a PDF")
        
        # Read PDF content
        pdf_content = await pdf_file.read()
        
        print(f"ð Processing PDF: {pdf_file.filename} ({len(pdf_content)} bytes)")
        
        # Extract text from PDF
        extracted_text = pdf_extractor.extract_text_from_pdf(pdf_content)
        
        if not extracted_text or len(extracted_text.strip()) < 50:
            raise HTTPException(status_code=400, detail="Could not extract sufficient text from PDF")
        
        print(f"ð Extracted {len(extracted_text)} characters from PDF")
        
        # Use enhanced verification system for PDF content
        analysis_result = await enhanced_verifier.analyze_text_authenticity(extracted_text)
        
        # Calculate additional metrics
        word_count = len(extracted_text.split())
        sentence_count = len([s for s in extracted_text.split('.') if s.strip()])
        processing_time = (datetime.now() - start_time).total_seconds()
        
        # Build enhanced verification result for PDF
        verification_result = {
            "fake_probability": analysis_result['fake_probability'],
            "is_likely_fake": analysis_result['fake_probability'] > 60,
            "confidence": "High" if analysis_result['fake_probability'] > 80 or analysis_result['fake_probability'] < 20 else "Medium",
            "detected_issues": analysis_result['detected_issues'],
            "ai_analysis": analysis_result['ai_analysis'],
            "structural_issues": analysis_result['structural_issues'],
            "quality_assessment": {
                "authenticity_score": analysis_result['quality_score'],
                "confidence_factors": analysis_result['confidence_factors'],
                "overall_quality": "Poor" if analysis_result['fake_probability'] > 70 else "Fair" if analysis_result['fake_probability'] > 40 else "Good"
            },
            "structure_analysis": {
                "word_count": word_count,
                "sentence_count": sentence_count,
                "avg_sentence_length": round(word_count / max(sentence_count, 1), 1),
                "has_proper_structure": word_count > 100 and sentence_count > 5,
                "extracted_text_length": len(extracted_text)
            },
            "recommendations": [
                "ð¨ High probability of fabricated content - manual review required" if analysis_result['fake_probability'] > 80 else
                "â ï¸ Suspicious content detected - verify claims independently" if analysis_result['fake_probability'] > 60 else
                "ï¿½ Some yconcerns identified - check statistical claims" if analysis_result['fake_probability'] > 40 else
                "â Content appears authentic with minor concerns" if analysis_result['fake_probability'] > 20 else
                "â Content appears highly authentic",
                
                "ð Verify all statistical claims independently" if any("stat" in issue["type"].lower() for issue in analysis_result['detected_issues']) else "ð Statistical reporting appears normal",
                
                "ð Check all citations and references" if any("citation" in issue["type"].lower() for issue in analysis_result['detected_issues']) else "ð Citation patterns appear normal",
                
                "ð¬ Review methodology for feasibility" if analysis_result['fake_probability'] > 50 else "ð¬ Methodology appears reasonable"
            ],
            "analysis_method": "enhanced_pdf_extraction_plus_ai_analysis",
            "processing_time_seconds": round(processing_time, 3),
            "timestamp": datetime.now().isoformat()
        }
        
        print(f"â Enhanced PDF verification completed in {processing_time:.3f}s - Fake probability: {analysis_result['fake_probability']}%")
        
        return {
            "success": True,
            "message": f"Enhanced PDF verification completed in {processing_time:.3f}s",
            "data": {
                "verification_result": verification_result,
                "metadata": {
                    "verified_at": start_time.isoformat(),
                    "file_name": pdf_file.filename,
                    "file_size": len(pdf_content),
                    "extracted_text_length": len(extracted_text),
                    "word_count": word_count,
                    "processing_time_seconds": round(processing_time, 3),
                    "performance": "enhanced-accuracy-pdf-analysis",
                    "detection_methods": ["statistical_analysis", "language_patterns", "ai_analysis", "structural_analysis", "quality_assessment", "pdf_text_extraction"]
                }
            }
        }
        
    except HTTPException:
        raise
    except Exception as e:
        print(f"â PDF verification failed: {e}")
        raise HTTPException(status_code=500, detail=f"PDF verification failed: {str(e)}")

if __name__ == "__main__":
    port = int(os.getenv("PORT", 8000))
    uvicorn.run(app, host="0.0.0.0", port=port, log_level="info")